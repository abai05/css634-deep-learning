{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x180 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "##########################\n",
    "### DATASET\n",
    "##########################\n",
    "\n",
    "ds = np.lib.DataSource()\n",
    "fp = ds.open('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data')\n",
    "\n",
    "x = np.genfromtxt(BytesIO(fp.read().encode()), delimiter=',', usecols=range(2), max_rows=100)\n",
    "y = np.zeros(100)\n",
    "y[50:] = 1\n",
    "\n",
    "np.random.seed(1)\n",
    "idx = np.arange(y.shape[0])\n",
    "np.random.shuffle(idx)\n",
    "X_test, y_test = x[idx[:25]], y[idx[:25]]\n",
    "X_train, y_train = x[idx[25:]], y[idx[25:]]\n",
    "mu, std = np.mean(X_train, axis=0), np.std(X_train, axis=0)\n",
    "X_train, X_test = (X_train - mu) / std, (X_test - mu) / std\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(7, 2.5))\n",
    "ax[0].scatter(X_train[y_train == 1, 0], X_train[y_train == 1, 1])\n",
    "ax[0].scatter(X_train[y_train == 0, 0], X_train[y_train == 0, 1])\n",
    "ax[1].scatter(X_test[y_test == 1, 0], X_test[y_test == 1, 1])\n",
    "ax[1].scatter(X_test[y_test == 0, 0], X_test[y_test == 0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class LogisticRegression():\n",
    "    def __init__(self, num_features):\n",
    "        self.num_features = num_features\n",
    "        self.weights = torch.zeros(num_features, 1, dtype=torch.float32, device=device)\n",
    "        self.bias = torch.zeros(1, dtype=torch.float32, device=device)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = torch.mm(x, self.weights) + self.bias      \n",
    "        a = self.__sigmoid(z)\n",
    "        return a\n",
    "    \n",
    "    def backward(self, a, y):\n",
    "        a = a.view(-1)\n",
    "        return a - y\n",
    "        \n",
    "    def evaluate(self, y, a):\n",
    "        pass\n",
    "    \n",
    "    def _logit_cost(self, y, proba):\n",
    "        tmp1 = torch.mm(-y.view(1, -1), torch.log(proba))\n",
    "        tmp2 = torch.mm((1 - y).view(1, -1), torch.log(1 - proba))\n",
    "        return tmp1 - tmp2\n",
    "        \n",
    "    def train(self, x, y, num_epochs, learning_rate):\n",
    "        for e in range(num_epochs):\n",
    "            # forward-backward propagation\n",
    "            a = self.forward(x)\n",
    "            dz = self.backward(a, y)\n",
    "            \n",
    "            # calculate gradients\n",
    "            dw = torch.mm(x.transpose(0, 1), dz.view(-1, 1))\n",
    "            db = torch.sum(dz)\n",
    "            \n",
    "            # update parameters\n",
    "            self.weights += learning_rate * dw\n",
    "            self.bias += learning_rate * db\n",
    "            \n",
    "            print('Epoch: %03d' % (e+1), end=' ')\n",
    "            print('Cost: %.3f' % self._logit_cost(y, self.forward(x)))\n",
    "            \n",
    "    \n",
    "    def __sigmoid(self, z):\n",
    "        return 1/(1 + torch.exp(-z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001Cost: 280.577\n"
     ]
    }
   ],
   "source": [
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32, device=device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32, device=device)\n",
    "\n",
    "model = LogisticRegression(num_features=2)\n",
    "model.train(X_train_tensor, y_train_tensor, num_epochs=1, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def custom_where(cond, x_1, x_2):\n",
    "    return (cond * x_1) + ((1-cond) * x_2)\n",
    "\n",
    "\n",
    "class LogisticRegression1():\n",
    "    def __init__(self, num_features):\n",
    "        self.num_features = num_features\n",
    "        self.weights = torch.zeros(num_features, 1, \n",
    "                                   dtype=torch.float32, device=device)\n",
    "        self.bias = torch.zeros(1, dtype=torch.float32, device=device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        linear = torch.add(torch.mm(x, self.weights), self.bias)\n",
    "        probas = self._sigmoid(linear)\n",
    "        return probas\n",
    "        \n",
    "    def backward(self, probas, y):  \n",
    "        errors = y - probas.view(-1)\n",
    "        return errors\n",
    "            \n",
    "    def predict_labels(self, x):\n",
    "        probas = self.forward(x)\n",
    "        labels = custom_where(probas >= .5, 1, 0)\n",
    "        return labels    \n",
    "            \n",
    "    def evaluate(self, x, y):\n",
    "        labels = self.predict_labels(x).float()\n",
    "        accuracy = torch.sum(labels.view(-1) == y) / y.size()[0]\n",
    "        return accuracy\n",
    "    \n",
    "    def _sigmoid(self, z):\n",
    "        return 1. / (1. + torch.exp(-z))\n",
    "    \n",
    "    def _logit_cost(self, y, proba):\n",
    "        tmp1 = torch.mm(-y.view(1, -1), torch.log(proba))\n",
    "        tmp2 = torch.mm((1 - y).view(1, -1), torch.log(1 - proba))\n",
    "        return tmp1 - tmp2\n",
    "    \n",
    "    def train(self, x, y, num_epochs, learning_rate=0.01):\n",
    "        for e in range(num_epochs):\n",
    "            \n",
    "            #### Compute outputs ####\n",
    "            probas = self.forward(x)\n",
    "#             print('Probas size:', probas.size())\n",
    "            \n",
    "            #### Compute gradients ####\n",
    "            errors = self.backward(probas, y)\n",
    "#             print('Errors size:', errors.size())\n",
    "            neg_grad = torch.mm(x.transpose(0, 1), errors.view(-1, 1))\n",
    "#             print('neg_grad:', neg_grad/x.size()[0])\n",
    "#             print('neg_grad size:', neg_grad.size())\n",
    "            \n",
    "            #### Update weights ####\n",
    "            self.weights += learning_rate * neg_grad\n",
    "            self.bias += learning_rate * torch.sum(errors)\n",
    "            \n",
    "            #### Logging ####\n",
    "            print('Epoch: %03d' % (e+1), end=\"\")\n",
    "            print(' | Train ACC: %.3f' % self.evaluate(x, y), end=\"\")\n",
    "            print(' | Cost: %.3f' % self._logit_cost(y, self.forward(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001 | Train ACC: 0.000 | Cost: 51.962\n",
      "Epoch: 002 | Train ACC: 0.000 | Cost: 51.937\n",
      "Epoch: 003 | Train ACC: 0.000 | Cost: 51.913\n",
      "Epoch: 004 | Train ACC: 0.000 | Cost: 51.890\n",
      "Epoch: 005 | Train ACC: 0.000 | Cost: 51.866\n",
      "Epoch: 006 | Train ACC: 0.000 | Cost: 51.843\n",
      "Epoch: 007 | Train ACC: 0.000 | Cost: 51.820\n",
      "Epoch: 008 | Train ACC: 0.000 | Cost: 51.797\n",
      "Epoch: 009 | Train ACC: 0.000 | Cost: 51.774\n",
      "Epoch: 010 | Train ACC: 0.000 | Cost: 51.752\n",
      "Epoch: 011 | Train ACC: 0.000 | Cost: 51.729\n",
      "Epoch: 012 | Train ACC: 0.000 | Cost: 51.707\n",
      "Epoch: 013 | Train ACC: 0.000 | Cost: 51.685\n",
      "Epoch: 014 | Train ACC: 0.000 | Cost: 51.663\n",
      "Epoch: 015 | Train ACC: 0.000 | Cost: 51.641\n",
      "Epoch: 016 | Train ACC: 0.000 | Cost: 51.620\n",
      "Epoch: 017 | Train ACC: 0.000 | Cost: 51.598\n",
      "Epoch: 018 | Train ACC: 0.000 | Cost: 51.577\n",
      "Epoch: 019 | Train ACC: 0.000 | Cost: 51.556\n",
      "Epoch: 020 | Train ACC: 0.000 | Cost: 51.535\n",
      "Epoch: 021 | Train ACC: 0.000 | Cost: 51.514\n",
      "Epoch: 022 | Train ACC: 0.000 | Cost: 51.493\n",
      "Epoch: 023 | Train ACC: 0.000 | Cost: 51.472\n",
      "Epoch: 024 | Train ACC: 0.000 | Cost: 51.452\n",
      "Epoch: 025 | Train ACC: 0.000 | Cost: 51.431\n",
      "Epoch: 026 | Train ACC: 0.000 | Cost: 51.411\n",
      "Epoch: 027 | Train ACC: 0.000 | Cost: 51.390\n",
      "Epoch: 028 | Train ACC: 0.000 | Cost: 51.370\n",
      "Epoch: 029 | Train ACC: 0.000 | Cost: 51.350\n",
      "Epoch: 030 | Train ACC: 0.000 | Cost: 51.330\n",
      "Epoch: 031 | Train ACC: 0.000 | Cost: 51.310\n",
      "Epoch: 032 | Train ACC: 0.000 | Cost: 51.290\n",
      "Epoch: 033 | Train ACC: 0.000 | Cost: 51.271\n",
      "Epoch: 034 | Train ACC: 0.000 | Cost: 51.251\n",
      "Epoch: 035 | Train ACC: 0.000 | Cost: 51.231\n",
      "Epoch: 036 | Train ACC: 0.000 | Cost: 51.212\n",
      "Epoch: 037 | Train ACC: 0.000 | Cost: 51.192\n",
      "Epoch: 038 | Train ACC: 0.000 | Cost: 51.173\n",
      "Epoch: 039 | Train ACC: 0.000 | Cost: 51.154\n",
      "Epoch: 040 | Train ACC: 0.000 | Cost: 51.135\n",
      "Epoch: 041 | Train ACC: 0.000 | Cost: 51.115\n",
      "Epoch: 042 | Train ACC: 0.000 | Cost: 51.096\n",
      "Epoch: 043 | Train ACC: 0.000 | Cost: 51.077\n",
      "Epoch: 044 | Train ACC: 0.000 | Cost: 51.058\n",
      "Epoch: 045 | Train ACC: 0.000 | Cost: 51.039\n",
      "Epoch: 046 | Train ACC: 0.000 | Cost: 51.020\n",
      "Epoch: 047 | Train ACC: 0.000 | Cost: 51.001\n",
      "Epoch: 048 | Train ACC: 0.000 | Cost: 50.983\n",
      "Epoch: 049 | Train ACC: 0.000 | Cost: 50.964\n",
      "Epoch: 050 | Train ACC: 0.000 | Cost: 50.945\n",
      "Epoch: 051 | Train ACC: 0.000 | Cost: 50.927\n",
      "Epoch: 052 | Train ACC: 0.000 | Cost: 50.908\n",
      "Epoch: 053 | Train ACC: 0.000 | Cost: 50.889\n",
      "Epoch: 054 | Train ACC: 0.000 | Cost: 50.871\n",
      "Epoch: 055 | Train ACC: 0.000 | Cost: 50.852\n",
      "Epoch: 056 | Train ACC: 0.000 | Cost: 50.834\n",
      "Epoch: 057 | Train ACC: 0.000 | Cost: 50.816\n",
      "Epoch: 058 | Train ACC: 0.000 | Cost: 50.797\n",
      "Epoch: 059 | Train ACC: 0.000 | Cost: 50.779\n",
      "Epoch: 060 | Train ACC: 0.000 | Cost: 50.761\n",
      "Epoch: 061 | Train ACC: 0.000 | Cost: 50.742\n",
      "Epoch: 062 | Train ACC: 0.000 | Cost: 50.724\n",
      "Epoch: 063 | Train ACC: 0.000 | Cost: 50.706\n",
      "Epoch: 064 | Train ACC: 0.000 | Cost: 50.688\n",
      "Epoch: 065 | Train ACC: 0.000 | Cost: 50.670\n",
      "Epoch: 066 | Train ACC: 0.000 | Cost: 50.652\n",
      "Epoch: 067 | Train ACC: 0.000 | Cost: 50.634\n",
      "Epoch: 068 | Train ACC: 0.000 | Cost: 50.616\n",
      "Epoch: 069 | Train ACC: 0.000 | Cost: 50.598\n",
      "Epoch: 070 | Train ACC: 0.000 | Cost: 50.580\n",
      "Epoch: 071 | Train ACC: 0.000 | Cost: 50.562\n",
      "Epoch: 072 | Train ACC: 0.000 | Cost: 50.544\n",
      "Epoch: 073 | Train ACC: 0.000 | Cost: 50.526\n",
      "Epoch: 074 | Train ACC: 0.000 | Cost: 50.508\n",
      "Epoch: 075 | Train ACC: 0.000 | Cost: 50.490\n",
      "Epoch: 076 | Train ACC: 0.000 | Cost: 50.472\n",
      "Epoch: 077 | Train ACC: 0.000 | Cost: 50.455\n",
      "Epoch: 078 | Train ACC: 0.000 | Cost: 50.437\n",
      "Epoch: 079 | Train ACC: 0.000 | Cost: 50.419\n",
      "Epoch: 080 | Train ACC: 0.000 | Cost: 50.401\n",
      "Epoch: 081 | Train ACC: 0.000 | Cost: 50.384\n",
      "Epoch: 082 | Train ACC: 0.000 | Cost: 50.366\n",
      "Epoch: 083 | Train ACC: 0.000 | Cost: 50.348\n",
      "Epoch: 084 | Train ACC: 0.000 | Cost: 50.331\n",
      "Epoch: 085 | Train ACC: 0.000 | Cost: 50.313\n",
      "Epoch: 086 | Train ACC: 0.000 | Cost: 50.295\n",
      "Epoch: 087 | Train ACC: 0.000 | Cost: 50.278\n",
      "Epoch: 088 | Train ACC: 0.000 | Cost: 50.260\n",
      "Epoch: 089 | Train ACC: 0.000 | Cost: 50.243\n",
      "Epoch: 090 | Train ACC: 0.000 | Cost: 50.225\n",
      "Epoch: 091 | Train ACC: 0.000 | Cost: 50.208\n",
      "Epoch: 092 | Train ACC: 0.000 | Cost: 50.190\n",
      "Epoch: 093 | Train ACC: 0.000 | Cost: 50.173\n",
      "Epoch: 094 | Train ACC: 0.000 | Cost: 50.155\n",
      "Epoch: 095 | Train ACC: 0.000 | Cost: 50.138\n",
      "Epoch: 096 | Train ACC: 0.000 | Cost: 50.120\n",
      "Epoch: 097 | Train ACC: 0.000 | Cost: 50.103\n",
      "Epoch: 098 | Train ACC: 0.000 | Cost: 50.086\n",
      "Epoch: 099 | Train ACC: 0.000 | Cost: 50.068\n",
      "Epoch: 100 | Train ACC: 0.000 | Cost: 50.051\n",
      "Epoch: 101 | Train ACC: 0.000 | Cost: 50.034\n",
      "Epoch: 102 | Train ACC: 0.000 | Cost: 50.016\n",
      "Epoch: 103 | Train ACC: 0.000 | Cost: 49.999\n",
      "Epoch: 104 | Train ACC: 0.000 | Cost: 49.982\n",
      "Epoch: 105 | Train ACC: 0.000 | Cost: 49.964\n",
      "Epoch: 106 | Train ACC: 0.000 | Cost: 49.947\n",
      "Epoch: 107 | Train ACC: 0.000 | Cost: 49.930\n",
      "Epoch: 108 | Train ACC: 0.000 | Cost: 49.913\n",
      "Epoch: 109 | Train ACC: 0.000 | Cost: 49.895\n",
      "Epoch: 110 | Train ACC: 0.000 | Cost: 49.878\n",
      "Epoch: 111 | Train ACC: 0.000 | Cost: 49.861\n",
      "Epoch: 112 | Train ACC: 0.000 | Cost: 49.844\n",
      "Epoch: 113 | Train ACC: 0.000 | Cost: 49.827\n",
      "Epoch: 114 | Train ACC: 0.000 | Cost: 49.810\n",
      "Epoch: 115 | Train ACC: 0.000 | Cost: 49.792\n",
      "Epoch: 116 | Train ACC: 0.000 | Cost: 49.775\n",
      "Epoch: 117 | Train ACC: 0.000 | Cost: 49.758\n",
      "Epoch: 118 | Train ACC: 0.000 | Cost: 49.741\n",
      "Epoch: 119 | Train ACC: 0.000 | Cost: 49.724\n",
      "Epoch: 120 | Train ACC: 0.000 | Cost: 49.707\n",
      "Epoch: 121 | Train ACC: 0.000 | Cost: 49.690\n",
      "Epoch: 122 | Train ACC: 0.000 | Cost: 49.673\n",
      "Epoch: 123 | Train ACC: 0.000 | Cost: 49.656\n",
      "Epoch: 124 | Train ACC: 0.000 | Cost: 49.639\n",
      "Epoch: 125 | Train ACC: 0.000 | Cost: 49.622\n",
      "Epoch: 126 | Train ACC: 0.000 | Cost: 49.605\n",
      "Epoch: 127 | Train ACC: 0.000 | Cost: 49.588\n",
      "Epoch: 128 | Train ACC: 0.000 | Cost: 49.571\n",
      "Epoch: 129 | Train ACC: 0.000 | Cost: 49.554\n",
      "Epoch: 130 | Train ACC: 0.000 | Cost: 49.537\n",
      "Epoch: 131 | Train ACC: 0.000 | Cost: 49.520\n",
      "Epoch: 132 | Train ACC: 0.000 | Cost: 49.503\n",
      "Epoch: 133 | Train ACC: 0.000 | Cost: 49.486\n",
      "Epoch: 134 | Train ACC: 0.000 | Cost: 49.469\n",
      "Epoch: 135 | Train ACC: 0.000 | Cost: 49.452\n",
      "Epoch: 136 | Train ACC: 0.000 | Cost: 49.435\n",
      "Epoch: 137 | Train ACC: 0.000 | Cost: 49.419\n",
      "Epoch: 138 | Train ACC: 0.000 | Cost: 49.402\n",
      "Epoch: 139 | Train ACC: 0.000 | Cost: 49.385\n",
      "Epoch: 140 | Train ACC: 0.000 | Cost: 49.368\n",
      "Epoch: 141 | Train ACC: 0.000 | Cost: 49.351\n",
      "Epoch: 142 | Train ACC: 0.000 | Cost: 49.334\n",
      "Epoch: 143 | Train ACC: 0.000 | Cost: 49.318\n",
      "Epoch: 144 | Train ACC: 0.000 | Cost: 49.301\n",
      "Epoch: 145 | Train ACC: 0.000 | Cost: 49.284\n",
      "Epoch: 146 | Train ACC: 0.000 | Cost: 49.267\n",
      "Epoch: 147 | Train ACC: 0.000 | Cost: 49.250\n",
      "Epoch: 148 | Train ACC: 0.000 | Cost: 49.234\n",
      "Epoch: 149 | Train ACC: 0.000 | Cost: 49.217\n",
      "Epoch: 150 | Train ACC: 0.000 | Cost: 49.200\n",
      "Epoch: 151 | Train ACC: 0.000 | Cost: 49.184\n",
      "Epoch: 152 | Train ACC: 0.000 | Cost: 49.167\n",
      "Epoch: 153 | Train ACC: 0.000 | Cost: 49.150\n",
      "Epoch: 154 | Train ACC: 0.000 | Cost: 49.133\n",
      "Epoch: 155 | Train ACC: 0.000 | Cost: 49.117\n",
      "Epoch: 156 | Train ACC: 0.000 | Cost: 49.100\n",
      "Epoch: 157 | Train ACC: 0.000 | Cost: 49.083\n",
      "Epoch: 158 | Train ACC: 0.000 | Cost: 49.067\n",
      "Epoch: 159 | Train ACC: 0.000 | Cost: 49.050\n",
      "Epoch: 160 | Train ACC: 0.000 | Cost: 49.034\n",
      "Epoch: 161 | Train ACC: 0.000 | Cost: 49.017\n",
      "Epoch: 162 | Train ACC: 0.000 | Cost: 49.000\n",
      "Epoch: 163 | Train ACC: 0.000 | Cost: 48.984\n",
      "Epoch: 164 | Train ACC: 0.000 | Cost: 48.967\n",
      "Epoch: 165 | Train ACC: 0.000 | Cost: 48.951\n",
      "Epoch: 166 | Train ACC: 0.000 | Cost: 48.934\n",
      "Epoch: 167 | Train ACC: 0.000 | Cost: 48.917\n",
      "Epoch: 168 | Train ACC: 0.000 | Cost: 48.901\n",
      "Epoch: 169 | Train ACC: 0.000 | Cost: 48.884\n",
      "Epoch: 170 | Train ACC: 0.000 | Cost: 48.868\n",
      "Epoch: 171 | Train ACC: 0.000 | Cost: 48.851\n",
      "Epoch: 172 | Train ACC: 0.000 | Cost: 48.835\n",
      "Epoch: 173 | Train ACC: 0.000 | Cost: 48.818\n",
      "Epoch: 174 | Train ACC: 0.000 | Cost: 48.802\n",
      "Epoch: 175 | Train ACC: 0.000 | Cost: 48.785\n",
      "Epoch: 176 | Train ACC: 0.000 | Cost: 48.769\n",
      "Epoch: 177 | Train ACC: 0.000 | Cost: 48.753\n",
      "Epoch: 178 | Train ACC: 0.000 | Cost: 48.736\n",
      "Epoch: 179 | Train ACC: 0.000 | Cost: 48.720\n",
      "Epoch: 180 | Train ACC: 0.000 | Cost: 48.703\n",
      "Epoch: 181 | Train ACC: 0.000 | Cost: 48.687\n",
      "Epoch: 182 | Train ACC: 0.000 | Cost: 48.670\n",
      "Epoch: 183 | Train ACC: 0.000 | Cost: 48.654\n",
      "Epoch: 184 | Train ACC: 0.000 | Cost: 48.638\n",
      "Epoch: 185 | Train ACC: 0.000 | Cost: 48.621\n",
      "Epoch: 186 | Train ACC: 0.000 | Cost: 48.605\n",
      "Epoch: 187 | Train ACC: 0.000 | Cost: 48.589\n",
      "Epoch: 188 | Train ACC: 0.000 | Cost: 48.572\n",
      "Epoch: 189 | Train ACC: 0.000 | Cost: 48.556\n",
      "Epoch: 190 | Train ACC: 0.000 | Cost: 48.540\n",
      "Epoch: 191 | Train ACC: 0.000 | Cost: 48.523\n",
      "Epoch: 192 | Train ACC: 0.000 | Cost: 48.507\n",
      "Epoch: 193 | Train ACC: 0.000 | Cost: 48.491\n",
      "Epoch: 194 | Train ACC: 0.000 | Cost: 48.474\n",
      "Epoch: 195 | Train ACC: 0.000 | Cost: 48.458\n",
      "Epoch: 196 | Train ACC: 0.000 | Cost: 48.442\n",
      "Epoch: 197 | Train ACC: 0.000 | Cost: 48.426\n",
      "Epoch: 198 | Train ACC: 0.000 | Cost: 48.409\n",
      "Epoch: 199 | Train ACC: 0.000 | Cost: 48.393\n",
      "Epoch: 200 | Train ACC: 0.000 | Cost: 48.377\n",
      "\n",
      "Model parameters:\n",
      "  Weights: tensor([[ 0.0721],\n",
      "        [-0.0631]])\n",
      "  Bias: tensor([-0.1307])\n"
     ]
    }
   ],
   "source": [
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32, device=device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32, device=device)\n",
    "\n",
    "logr = LogisticRegression1(num_features=2)\n",
    "logr.train(X_train_tensor, y_train_tensor, num_epochs=200, learning_rate=0.001)\n",
    "\n",
    "print('\\nModel parameters:')\n",
    "print('  Weights: %s' % logr.weights)\n",
    "print('  Bias: %s' % logr.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
